import os
from langchain_community.document_loaders import WikipediaLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# 1. ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ë¡œë“œ
# 'ëŒ€í•œë¯¼êµ­' í˜ì´ì§€ì˜ ì½˜í…ì¸ ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
def wiki_data_load( query: str, search: str):
    print(f"ğŸ“œ {search} ìœ„í‚¤í”¼ë””ì•„ í˜ì´ì§€ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")
    loader = WikipediaLoader(search, lang='ko', load_max_docs=1)
    documents = loader.load()
    print(f"âœ… ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    print("\nâœ‚ï¸ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    docs = text_splitter.split_documents(documents)
    print(f"âœ… í…ìŠ¤íŠ¸ë¥¼ ì´ {len(docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    # 3. ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
    # sentence-transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    # 'jhgan/ko-sroberta-multitask'ëŠ” í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©ì— íŠ¹í™”ëœ ëª¨ë¸ì…ë‹ˆë‹¤.
    print("\nğŸ§  ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤ (ìµœì´ˆ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    model_name = "jhgan/ko-sroberta-multitask"
    model_kwargs = {'device': 'cuda'}  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
    encode_kwargs = {'normalize_embeddings': True}  # ë²¡í„° ì •ê·œí™”
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    print(f"âœ… '{model_name}' ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    # 4. ë²¡í„° DB ìƒì„± ë° ì €ì¥
    # ì¤€ë¹„ëœ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  FAISS DBì— ì €ì¥í•©ë‹ˆë‹¤.
    print("\nğŸ’¿ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©í•˜ê³  FAISS DBì— ì €ì¥í•©ë‹ˆë‹¤...")
    db = FAISS.from_documents(docs, embeddings)
    print("âœ… FAISS ë²¡í„° DB ìƒì„± ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 5. ë²¡í„° DB ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # ì €ì¥ëœ ë²¡í„° DBë¥¼ í™œìš©í•˜ì—¬ íŠ¹ì • ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    print("\nğŸ” ì €ì¥ëœ ë²¡í„° DBì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
    retrieved_docs = db.similarity_search(query, k=1)

    print(f"\n[ì§ˆë¬¸]: {query}")
    ans = []
    for i, doc in enumerate(retrieved_docs):
        ans.append(doc.page_content)
    return ans    